{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e69d743a-7c8e-4fb6-8ccd-64f9c2d88e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "#Copula approach\n",
    "    #Step 1: filter pairs based on kendall tau\n",
    "    #step 2: find best fit distribution\n",
    "    #Step 3: find best fit copula\n",
    "    #Step 4: use copula formula to calculate conditional probability and create trading strategy\n",
    "    #Step 5: get results\n",
    "#edits:\n",
    "    #added rolling window to cond prob calculation to avoid future bias\n",
    "    #added position shift like other approach\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from copulae import GaussianCopula, ClaytonCopula, GumbelCopula\n",
    "import warnings\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Select a wide range of assets you're interested in\n",
    "assets = ['ES=F', 'CL=F', 'GC=F', 'SI=F', 'AAPL', 'MSFT', 'GOOGL', 'AMZN', \n",
    "          'JPM', 'V', 'PG', 'XOM', 'GLD', 'SLV', 'USO', 'SPY', 'TLT', 'QQQ', 'DIA', \n",
    "          'NFLX', 'TSLA', 'BA', 'KO', 'DIS', 'PLUG', 'GS', 'IBM', 'INTC', 'ORCL', 'NVDA']\n",
    "\n",
    "start_date = '2000-01-01'\n",
    "end_date = '2021-12-31'\n",
    "\n",
    "# Fetch the data for all assets\n",
    "asset_data = []\n",
    "asset_dict = {}\n",
    "for i,  asset in enumerate(assets):\n",
    "    try:\n",
    "        df = yf.download(asset, start=start_date, end=end_date)\n",
    "        if not df.empty:  # If df is not empty\n",
    "            df = df.resample('B').mean().fillna(method='ffill')  # Resample at daily frequency\n",
    "            asset_data.append(df['Close'])\n",
    "            asset_dict[asset] = i\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {asset}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6943ab4-99fe-4b5e-921b-926c1bba1bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 1: ES=F and SPY with ktcorr 0.8898432086383968\n",
      "Pair 2: CL=F and USO with ktcorr 0.8253174375037056\n",
      "Pair 3: ES=F and DIA with ktcorr 0.787541014847115\n",
      "Pair 4: GC=F and GLD with ktcorr 0.7441352113386912\n",
      "Pair 5: ES=F and QQQ with ktcorr 0.6674910264566074\n",
      "Pair 6: GC=F and SI=F with ktcorr 0.586583621182953\n",
      "Pair 7: ES=F and JPM with ktcorr 0.5313047901130323\n",
      "Pair 8: ES=F and GS with ktcorr 0.5115093273965535\n",
      "Pair 9: ES=F and MSFT with ktcorr 0.47730884944224294\n",
      "Pair 10: ES=F and DIS with ktcorr 0.47522514227334767\n",
      "Pair 11: ES=F and IBM with ktcorr 0.4741621621302099\n",
      "Pair 12: ES=F and INTC with ktcorr 0.4661730014111685\n",
      "Pair 13: ES=F and ORCL with ktcorr 0.45427688270914895\n",
      "Pair 14: ES=F and XOM with ktcorr 0.44878829544263743\n",
      "Pair 15: ES=F and BA with ktcorr 0.42623490422497323\n",
      "Pair 16: ES=F and V with ktcorr 0.4189230727819263\n",
      "Pair 17: ES=F and GOOGL with ktcorr 0.39828410872723097\n",
      "Pair 18: ES=F and AMZN with ktcorr 0.38625532970828885\n",
      "Pair 19: ES=F and AAPL with ktcorr 0.38394093338793983\n",
      "Pair 20: ES=F and NVDA with ktcorr 0.3826388542813677\n"
     ]
    }
   ],
   "source": [
    "#Apply testing period for assets\n",
    "test_start = pd.Timestamp(\"2000-01-01\")\n",
    "test_end = pd.Timestamp(\"2019-12-31\")\n",
    "\n",
    "#Step 1: Select pairs with highest kendall tau\n",
    "    #A statistic used to measure the ordinal association between two measured quantities\n",
    "selected_pairs = []\n",
    "# Perform analysis on each asset pair and select the pairs\n",
    "for i in range(len(asset_data)):\n",
    "    for j in range(i+1, len(asset_data)):\n",
    "        asset1 = asset_data[i].loc[test_start:test_end]\n",
    "        asset2 = asset_data[j].loc[test_start:test_end]\n",
    "        prices1 = asset1\n",
    "        prices2 = asset2\n",
    "        returns1 = np.log(prices1).diff().dropna()\n",
    "        returns2 = np.log(prices2).diff().dropna()\n",
    "       \n",
    "        # Ensure returns1 and returns2 have the same length\n",
    "        min_len = min(len(returns1), len(returns2))\n",
    "        returns1 = returns1[:min_len]\n",
    "        returns2 = returns2[:min_len]\n",
    "   \n",
    "        # Align the two return series on their index: due to missing data\n",
    "        aligned_returns1, aligned_returns2 = returns1.align(returns2, join='inner')\n",
    "        ktcorr = stats.kendalltau(aligned_returns1, aligned_returns2)[0]\n",
    "\n",
    "\n",
    "        #filter pairs based on ktcorr>0.35\n",
    "        if ktcorr > 0.35:\n",
    "            selected_pairs.append((assets[i], assets[j], ktcorr))\n",
    "\n",
    "selected_pairs = selected_pairs[:20]\n",
    "\n",
    "# Sort the filtered pairs based on ktcorr in ascending order\n",
    "selected_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "for i, pair in enumerate(selected_pairs):\n",
    "    print(f\"Pair {i+1}: {pair[0]} and {pair[1]} with ktcorr {pair[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "474f7fe2-21e5-4a98-8421-ed3d4c2d649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Select marginal distribution using the Akaike Information Criterion (AIC) and filter based on those with student t\n",
    "    #AIC is used to compare different possible models and determine which one is the best fit for the data\n",
    "    #is calculated from\n",
    "        #the number of independent variables used to build the model\n",
    "        #the maximum likelihood estimate of the model\n",
    "def compute_best_dist(data):\n",
    "    dists = ['Normal', \"Student's t\", 'Logistic', 'Extreme']\n",
    "    best_aic = np.inf #compare and seek the min AIC value\n",
    "    for dist, name in zip([stats.norm, stats.t, stats.genlogistic, stats.genextreme], dists):\n",
    "        params = dist.fit(data)\n",
    "        dist_fit = dist(*params)\n",
    "        log_like = np.log(dist_fit.pdf(data)).sum()\n",
    "        aic = 2*len(params) - 2 * log_like\n",
    "        if aic<best_aic:\n",
    "            best_dist = name\n",
    "            best_aic = aic\n",
    "    return [best_dist]\n",
    "\n",
    "\n",
    "# Dictionary to store the best fitting distribution for each asset\n",
    "asset_dist_dict = {}\n",
    "for asset1, asset2, ktcorr in selected_pairs:\n",
    "\n",
    "    # Calculate the best fitting distribution for each asset's returns\n",
    "    asset_dist_dict[asset1] = compute_best_dist(returns1)\n",
    "    asset_dist_dict[asset2] = compute_best_dist(returns2)\n",
    "     # Check if both assets' returns best fit to Student's t distribution\n",
    "    if compute_best_dist(returns1) == \"Normal\" and compute_best_dist(returns2) == \"Normal\":\n",
    "        selected_pairs.append((asset1, asset2, ktcorr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "364a8a09-1789-432c-8c81-dc22cc707acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES=F and SPY: GumbelCopula\n",
      "CL=F and USO: GumbelCopula\n",
      "ES=F and DIA: GumbelCopula\n",
      "GC=F and GLD: GumbelCopula\n",
      "ES=F and QQQ: GumbelCopula\n",
      "GC=F and SI=F: GumbelCopula\n",
      "ES=F and JPM: GumbelCopula\n",
      "ES=F and GS: GumbelCopula\n",
      "ES=F and MSFT: GumbelCopula\n",
      "ES=F and DIS: GumbelCopula\n",
      "ES=F and IBM: GumbelCopula\n",
      "ES=F and INTC: GumbelCopula\n",
      "ES=F and ORCL: GumbelCopula\n",
      "ES=F and XOM: GumbelCopula\n",
      "ES=F and BA: GumbelCopula\n",
      "ES=F and V: GumbelCopula\n",
      "ES=F and GOOGL: GumbelCopula\n",
      "ES=F and AMZN: GumbelCopula\n",
      "ES=F and AAPL: GumbelCopula\n",
      "ES=F and NVDA: GumbelCopula\n"
     ]
    }
   ],
   "source": [
    "#trading period\n",
    "trade_start = pd.Timestamp(\"2020-01-01\")\n",
    "trade_end = pd.Timestamp(\"2020-12-31\")\n",
    "# Initialize empty lists for storing calculated values\n",
    "portfolio_returns = []\n",
    "portfolio_volatility = []\n",
    "for asset1, asset2, ktcorr in selected_pairs:    # Get the returns of the testing period\n",
    "    # Get the returns of the trading period\n",
    "    asset1_trade = asset_data[asset_dict[asset1]].loc[start_date:end_date]\n",
    "    asset2_trade = asset_data[asset_dict[asset2]].loc[start_date:end_date]\n",
    "    prices1_trade = asset1_trade\n",
    "    prices2_trade = asset2_trade\n",
    "    returns1_trade = np.log(prices1_trade).diff().dropna()\n",
    "    returns2_trade = np.log(prices2_trade).diff().dropna()\n",
    "    # Ensure returns1_trade and returns2_trade have the same length\n",
    "    min_len = min(len(returns1_trade), len(returns2_trade))\n",
    "    returns1_trade = returns1_trade[:min_len]\n",
    "    returns2_trade = returns2_trade[:min_len]\n",
    "   \n",
    "    # fit marginals\n",
    "    params_s1 = stats.t.fit(returns1)\n",
    "    dist_s1 = stats.t(*params_s1)\n",
    "    params_s2 = stats.t.fit(returns2)\n",
    "    dist_s2 = stats.t(*params_s2)\n",
    "   \n",
    "    # transform marginals\n",
    "    u = dist_s1.cdf(returns1)\n",
    "    v = dist_s2.cdf(returns2)\n",
    "    min_len = min(len(u), len(v))\n",
    "    u = u[:min_len]\n",
    "    v = v[:min_len]\n",
    "    data = np.column_stack([u, v])\n",
    "\n",
    "\n",
    "    #Step 3: Select best fitting copula\n",
    "    best_aic = np.inf\n",
    "    copulas = [GaussianCopula(dim=2), ClaytonCopula(dim=2), GumbelCopula(dim=2)]\n",
    "    for copula in copulas:\n",
    "        copula.fit(data)\n",
    "        L = copula.log_lik(data)\n",
    "        aic = 2 * 1 - 2 * L  # copula models have 1 parameter\n",
    "        if aic < best_aic:\n",
    "            best_aic = aic\n",
    "            best_copula = copula\n",
    "            \n",
    "    copula_name = str(type(best_copula)).split('.')[-1].replace(\"'>\", '')\n",
    "    print(f'{asset1} and {asset2}: {copula_name}')\n",
    "        # Since all pairs best copula is Gumbel can proceed using its formulae \n",
    "    #Step 4: calculate conditional probability and create trading positions        \n",
    "    # Create positions for trading period\n",
    "    positions1 = []\n",
    "    positions2 = []\n",
    "    u = returns1_trade.apply(dist_s1.cdf).rolling(30).mean().dropna()\n",
    "    v = returns2_trade.apply(dist_s2.cdf).rolling(30).mean().dropna()\n",
    "    # Getting the min length from u, v and returns to align them\n",
    "    min_len = min(len(u), len(v), len(returns1_trade), len(returns2_trade))\n",
    "    u = u[:min_len]\n",
    "    v = v[:min_len]\n",
    "    returns1_trade = returns1_trade[:min_len]\n",
    "    returns2_trade = returns2_trade[:min_len]\n",
    "\n",
    "    #Best copula model for all pairs is Gumbel, so calculate conditional probability based on gumbel copula formula\n",
    "    #Conditional probability u given v (C(u|v) for Gumbel copula)\n",
    "        #Formula: C(u|v; θ) = exp(-((-log(v))^θ + (-log(u))^θ)^(1/θ) / v)\n",
    "            #theta is the copula parameter\n",
    "    theta = best_copula.params  # copula parameter\n",
    "    # Conditional probability u given v for Gumbel copula\n",
    "    prob_u_given_v = np.exp(-((-np.log(v))**theta + (-np.log(u))**theta)**(1/theta)) / v\n",
    "    # Conditional probability v given u for Gumbel copula\n",
    "    prob_v_given_u = np.exp(-((-np.log(u))**theta + (-np.log(v))**theta)**(1/theta)) / u\n",
    "    threshold = 0.50\n",
    "\n",
    "    # Create positions based on the conditional probabilities\n",
    "    positions1 = [1 if p > threshold else -1 for p in prob_u_given_v]\n",
    "    positions2 = [1 if p > threshold else -1 for p in prob_v_given_u]\n",
    "    # Getting the min length from positions1, positions2 and returns to align them\n",
    "    min_len = min(len(positions1), len(positions2), len(returns1_trade), len(returns2_trade))\n",
    "    positions1 = positions1[:min_len]\n",
    "    positions2 = positions2[:min_len]\n",
    "    returns1_trade = returns1_trade[:min_len]\n",
    "    returns2_trade = returns2_trade[:min_len]\n",
    "    positions1 = pd.Series(positions1, index=returns1_trade.index)\n",
    "    positions2 = pd.Series(positions2, index=returns2_trade.index)\n",
    "    returns1_from_positions = returns1_trade * positions1.shift(2)\n",
    "    returns2_from_positions = returns2_trade * positions2.shift(2)\n",
    "\n",
    "    # Combine returns of the two assets\n",
    "    total_returns = returns1_from_positions + returns2_from_positions\n",
    "    # Calculate the average return\n",
    "    total_return_mean = total_returns.mean()\n",
    "    # Calculate the standard deviation of the returns\n",
    "    std_dev = total_returns.std()\n",
    "    portfolio_returns.append(total_return_mean)\n",
    "    portfolio_volatility.append(std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a4dc79c-33d1-47bd-b368-d2a5742482a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 1: ES=F, SPY, Sharpe Ratio: 0.2771908347015686, Copula: GumbelCopula, Kendall Tau correlation: 0.8898432086383968\n",
      "Pair 2: CL=F, USO, Sharpe Ratio: -0.23692609867369413, Copula: GumbelCopula, Kendall Tau correlation: 0.8253174375037056\n",
      "Pair 3: ES=F, DIA, Sharpe Ratio: 0.295866348008755, Copula: GumbelCopula, Kendall Tau correlation: 0.787541014847115\n",
      "Pair 4: GC=F, GLD, Sharpe Ratio: 0.19466305790360694, Copula: GumbelCopula, Kendall Tau correlation: 0.7441352113386912\n",
      "Pair 5: ES=F, QQQ, Sharpe Ratio: 0.2761208694208075, Copula: GumbelCopula, Kendall Tau correlation: 0.6674910264566074\n",
      "Pair 6: GC=F, SI=F, Sharpe Ratio: 0.3544500023238844, Copula: GumbelCopula, Kendall Tau correlation: 0.586583621182953\n",
      "Pair 7: ES=F, JPM, Sharpe Ratio: 0.22619012627443316, Copula: GumbelCopula, Kendall Tau correlation: 0.5313047901130323\n",
      "Pair 8: ES=F, GS, Sharpe Ratio: 0.2154038549166779, Copula: GumbelCopula, Kendall Tau correlation: 0.5115093273965535\n",
      "Pair 9: ES=F, MSFT, Sharpe Ratio: 0.34539040752320266, Copula: GumbelCopula, Kendall Tau correlation: 0.47730884944224294\n",
      "Pair 10: ES=F, DIS, Sharpe Ratio: 0.3064096197433893, Copula: GumbelCopula, Kendall Tau correlation: 0.47522514227334767\n",
      "Pair 11: ES=F, IBM, Sharpe Ratio: 0.14553097403773316, Copula: GumbelCopula, Kendall Tau correlation: 0.4741621621302099\n",
      "Pair 12: ES=F, INTC, Sharpe Ratio: 0.11441412917961041, Copula: GumbelCopula, Kendall Tau correlation: 0.4661730014111685\n",
      "Pair 13: ES=F, ORCL, Sharpe Ratio: 0.22119648324200697, Copula: GumbelCopula, Kendall Tau correlation: 0.45427688270914895\n",
      "Pair 14: ES=F, XOM, Sharpe Ratio: 0.19001422311156288, Copula: GumbelCopula, Kendall Tau correlation: 0.44878829544263743\n",
      "Pair 15: ES=F, BA, Sharpe Ratio: 0.2687645458783146, Copula: GumbelCopula, Kendall Tau correlation: 0.42623490422497323\n",
      "Pair 16: ES=F, V, Sharpe Ratio: -0.40784681250282084, Copula: GumbelCopula, Kendall Tau correlation: 0.4189230727819263\n",
      "Pair 17: ES=F, GOOGL, Sharpe Ratio: 0.16354839487721112, Copula: GumbelCopula, Kendall Tau correlation: 0.39828410872723097\n",
      "Pair 18: ES=F, AMZN, Sharpe Ratio: 0.4634099371489684, Copula: GumbelCopula, Kendall Tau correlation: 0.38625532970828885\n",
      "Pair 19: ES=F, AAPL, Sharpe Ratio: 0.5543643416503794, Copula: GumbelCopula, Kendall Tau correlation: 0.38394093338793983\n",
      "Pair 20: ES=F, NVDA, Sharpe Ratio: 0.4635828069099804, Copula: GumbelCopula, Kendall Tau correlation: 0.3826388542813677\n"
     ]
    }
   ],
   "source": [
    "#Step 5: get results\n",
    "for i, (asset1, asset2, ktcorr) in enumerate(selected_pairs):\n",
    "    sharpe_ratio = np.sqrt(252) * portfolio_returns[i] / (portfolio_volatility[i])\n",
    "    print(f\"Pair {i+1}: {asset1}, {asset2}, Sharpe Ratio: {sharpe_ratio}, Copula: {copula_name}, Kendall Tau correlation: {ktcorr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "727f1c06-03c3-4e58-9069-14ee1778e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best perfoming pair is ES=F and AAPL with sharpe ratio of 0.554\n",
    "# Limitations\n",
    "    # Only relied on pairs with Normal distribution\n",
    "    # Limited Asset Pair Selection: The code only filters asset pairs based on the Kendall Tau correlation above 0.35\n",
    "    # Lack of Transaction Cost Consideration: The code assumes no transaction costs\n",
    "    # Lack of Risk Management: The code doesn't consider drawdown or risk-adjusted returns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
